#+TITLE: Chapter 13: Statistics

This chapter has 4 focal points:

- Normality Tests

A large number of important financial models, like modern or mean-variance portfolio theory (MPT) and the capital asset pricing model (CAPM) rest on the assumption that returns of securities are normally distributed. This chapter presents approaches to testing a given time series for normality of returns

- Portfolio Optimization

Starting in the early 1950s with the work of pioneer Harry Markowitz, *Mean-Variance Portfolio Theory* began to replace people's reliance on judgement and experience with rigorous mathematical and statistical methods when it comes to the investment of money in financial markets.

- Bayesian Statistics

Bayesian statistics introduces the notion of beliefs of agents and the updating of beliefs to statistics.

- Machine Learning

This section looks at supervised learning for classification

* Normality Tests

Among others, the following cornerstones of financial theory rest to a large extend on the assumption that returns of a financial instrument are normally distributed:

- Portfolio Theory
  When stock returns are normally distributed, optimal portfolio choice can be cast into a setting where only the (expected) /mean return/ and the /variance of the returns/ (or the volatility) as well as the /covariances/ between different stocks are relevant for an investment decision (i.e., an optimal portfolio composition).

- Capital Asset Pricing Model
  When stock returns are normally distributed, prices of single stocks can be elegantly expressed in a linear relationship to a broad market index; the relationship is generally expressed by a measure for the co-movement of a single stock with the market index called beta or $\beta$

- Efficient Markets Hypothesis
  An efficient market is a market where prices reflect all available information, where "all" can be defined more narrowly or more widely (e.g., as in "all publicly available" information vs. including also "only privately available" information). If this hypothesis holds true, then stock prices fluctuate randomly and returns are normally distributed

- Option pricing theory
  Brownian motion is /the/ benchmark model for the modeling of random price movements of financial instruments; the famous Black-Scholes-Merton option pricing formula uses a geometric Brownian motion as the model for a stock's random price fluctuations over time, leading to log-normally distributed prices and normally distributed returns.

** Benchmark Case

The analysis starts with GBM as one of the canonical stochastic processes used in financial modeling.

The following can be said about the characteristics of paths from a geometric Brownian motion $S$:

- Normal log returns

Log returns $\log{\frac{S_t}{S_s}} = \log{S_t} - \log{S_s}$ between two times $0 < s < t$ are normally distributed

- Log-normal values

At any time $t > 0$, the values $S_t$ are log-normally distributed.

#+begin_src python
import math
import numpy as np
import scipy.stats as scs
import statsmodels.api as sm
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('tkAgg')
plt.style.use('seaborn')

def gen_paths(S0, r, sigma, T, M, I):
    """
    Generate Monte Carlo paths for geometric Brownian motion

    Args:
        S0: initial stock/index value
        r : constant short rate
        sigma: constant volatility
        T : final time horizon
        M : number of time steps / intervals
        I : number of paths to be simulated

    Returns:
        paths: simulated paths given the parameters
    """
    dt = T / M
    paths = np.zeros((M + 1, I))
    paths[0] = S0
    for t in range(1, M + 1):
        rand = np.random.standard_normal(I)
        # standardize / match 1st and 2nd moment
        rand = (rand - rand.mean()) / rand.std()
        # vectorized Euler discretization of geometric Brownian motion
        paths[t] = paths[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt +
                                         sigma * math.sqrt(dt) * rand)
    return paths

S0 = 100.
r = 0.05
sigma = 0.2
T = 1.0
M = 50
I = 250000
np.random.seed(1000)

paths = gen_paths(S0, r, sigma, T, M, I)

S0 * math.exp(r * T)  # expected value and average simulated value

paths[-1].mean()

# ten simulated paths of geometric Brownian motion
plt.figure()
plt.plot(paths[:, :10])
plt.xlabel('time steps')
plt.ylabel('index level')
plt.show()

log_returns = np.log(paths[1:] / paths[:-1]).round(4)

def print_statistics(array):
    """
    prints selected statistics
    """
    sta = scs.describe(array)
    print('%14s %15s' % ('statistic', 'value'))
    print(30 * '-')
    print('%14s %15.5f' % ('size', sta[0]))
    print('%14s %15.5f' % ('min', sta[1][0]))
    print('%14s %15.5f' % ('max', sta[1][1]))
    print('%14s %15.5f' % ('mean', sta[2]))
    print('%14s %15.5f' % ('std', np.sqrt(sta[3])))
    print('%14s %15.5f' % ('skew', sta[4]))
    print('%14s %15.5f' % ('kurtosis', sta[5]))

print_statistics(log_returns.flatten())

log_returns.mean() * M + 0.5 * sigma ** 2  # annualized mean log return after correction for the Ito term

log_returns.std() * math.sqrt(M)  # annualized volatility, i.e., annualized std dev of log returns

# histogram of log returns of geometric Brownian motion and normal density function
plt.figure()
plt.hist(log_returns.flatten(), bins=70, density=True, label='frequency', color='b')
plt.xlabel('log return')
plt.ylabel('frequency')
x = np.linspace(plt.axis()[0], plt.axis()[1])
plt.plot(x, scs.norm.pdf(x, loc=r/M, scale=sigma/np.sqrt(M)))
plt.show()
#+end_src

Comparing a frequency distribution (histogram) with a theoretical PDF is not the only way to graphically "test" for normality. Quantile-quantile (QQ) plots are also well-suited for this task.

#+begin_src python
sm.qqplot(log_returns.flatten()[::500], line='s')
plt.xlabel('theoretical quantiles')
plt.ylabel('sample quantiles')
plt.show()
#+end_src

However appealing graphical tests are, they generally cannot replace more rigorous testing procedures.

- skewness test (skewtest())
  tests whether the skew (3rd moment) of the sample data is normal
- kurtosis test (kurtosistest())
- normality test (normaltest())

#+begin_src python
def normality_tests(arr):
    """
    Tests for normality distribution of a given data set
    Args:
        arr: object to generate statistics on
    Side Effect:
        prints a bunch of stats to the console
    """
    print(f'Skew:\t {scs.skew(arr):30.3f}')
    print(f'Skew test p_value:\t {scs.skewtest(arr)[1]:14.3f}')
    print(f'Kurt of data set:\t {scs.kurtosis(arr):14.3f}')
    print(f'Kurt test p-value:\t {scs.kurtosistest(arr)[1]:14.3f}')
    print(f'Norm test p-value:\t {scs.normaltest(arr)[1]:14.3f}')

normality_tests(log_returns.flatten())

# histogram of simualted end-of-period index levels for geometric Brownian motion
f, (ax1, ax2) = plt.subplots(1, 2)
ax1.hist(paths[-1], bins=30)
ax1.set_xlabel('index level')
ax1.set_ylabel('frequency')
ax1.set_title('regular data')
ax2.hist(np.log(paths[-1]), bins=30)
ax2.set_xlabel('log index level')
ax2.set_title('log data')
plt.show()
#+end_src


** Real-World Data

This section analyzes 4 historical financial time series: apple, microsoft, S&P 500, SPDR Gold

#+begin_src python
import pandas as pd
raw = pd.read_csv('data/tr_eikon_eod_data.csv',
                  index_col=0, parse_dates=True).dropna()
symbols = ['SPY', 'GLD', 'AAPL.O', 'MSFT.O']

data = raw[symbols]

data.info()
data.head()

# normalized prices of financial instruments over time
(data / data.iloc[0] * 100).plot()
plt.show()

# histograms of log returns for financial instruments
log_returns = np.log(data / data.shift(1))
log_returns.head()

log_returns.hist(bins=50)
plt.show()

for sym in symbols:
    print(f'\nResults for symbol {sym}')
    print(30 * '-')
    log_data = np.array(log_returns[sym].dropna())
    print_statistics(log_data)
    print('\nNormality Tests\n')
    normality_tests(log_data)
#+end_src

* Portfolio Optimization

** The Data

The basic idea of MPT is to make use of /diversification/ to achieve a minimal portfolio risk given a target return level or a maximum portfolio return given a certain level of risk. One would expect such diversification effects for the right combination of a larger number of assets and a certain diversity in the assets.

#+begin_src python
noa = len(symbols)  # number of assets

rets = np.log(data / data.shift(1))
rets.hist(bins=40)
plt.show()
#+end_src

The /covariance matrix/ for the financial instruments to be invested in is the central piece of the portfolio selection process.

#+begin_src python
rets.mean() * 252  # annualized mean returns
rets.cov() * 252   # annualized covariance matrix
#+end_src

** The Basic Theory

In what follows, it is assumed that an investor is not allowed to set up short positions in a financial instrument. Only long positions are allowed, which implies that 100% of the investor's wealth has to be divided among the available instruments in such a way that all positions are long (positive) and that the positions add up to 100%.

#+begin_src python
# gen 4 uniformly distributed random nums and normalize s.t. sum(rvs) = 1
weights = np.random.random(noa)
weights /= np.sum(weights)
#+end_src

General formula for expected portfolio return

$\mu_p = E(\Sigma_I w_i r_i) = w^T \mu$

#+begin_src python
np.sum(rets.mean() * weights) * 252  # annualized portfolio return given the portfolio weights
#+end_src

The second object of importance in MPT is the /expected portfolio variance/.

General formula for expected portfolio variance
$\sigma_p^2 = E((r - \mu)^2) = w^T \Sigma w$

#+begin_src python
portfolio_cov = np.dot(weights.T, np.dot(rets.cov() * 252, weights))
volatility = math.sqrt(portfolio_cov)
#+end_src

Of paramount interest to investors is what risk-return profiles are possible for a given set of financial instruments, and their statistical characteristics.

The following implements a Monte Carlo simulation to generate random portfolio weight vectors on a larger scale. For every simulated allocation, the code records the resulting expected portfolio return and variance.

#+begin_src python
def port_ret(weights):
    return np.sum(rets.mean() * weights) * 252

def port_vol(weights):
    return np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))

# monte carlo simulation of portfolio weights
prets, pvols = [], []
for p in range(2500):
    weights = np.random.random(noa)
    weights /= np.sum(weights)
    prets.append(port_ret(weights))
    pvols.append(port_vol(weights))
prets = np.array(prets)
pvols = np.array(pvols)

# expected return and volatility for random portfolio weights
plt.figure()
plt.scatter(pvols, prets, c=prets / pvols,
            marker='o', cmap='coolwarm')
plt.xlabel('expected volatility')
plt.ylabel('expected return')
plt.colorbar(label='Sharpe ratio')
plt.show()
#+end_src

The plot above illustrates the results of the Monte Carlo simulation. It also provides results for the *Sharpe ratio*, defined as:

$SR \equiv \frac{\mu_p - r_f}{\sigma_p}$

i.e., the expected excess return of the portfolio over the risk-free short rate divided by the expected standard deviation of the portfolio.
For simplicity, in this casae $r_f \equiv 0$ is assumed.

As an investor, one is generally interested in the maximum return given a fixed risk level or the minimum risk given a fixed return expectation.
This set of portfolios then makes up the /efficient frontier/.

** Optimal Portfolios

This /minimization/ function is quite general and allows for equality constraints, inequality constrants, and numerical bounds for the parameters.

First, the /maximization of the Sharpe ratio/. Formally, the negative value of the Sharpe ratio is minimized to derive at the maximum value and the optimal portfolio composition. The constraint is that all the weights add up to 1.

#+begin_src python
import scipy.optimize as sco

def min_func_sharpe(weights):
    # function to be minimized
    return -port_ret(weights) / port_vol(weights)

cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})  # equality constraint
bnds = tuple((0, 1) for x in range(noa))  # bounds for the parameters
eweights = np.array(noa * [1. / noa,])  # equal weights vector

opts = sco.minimize(min_func_sharpe,
                    eweights,
                    method='SLSQP',
                    bounds=bnds,
                    constraints=cons)

# maximize Sharpe ratio
optimals = opts['x'].round(3)  # optimal portfolio weights
port_ret(optimals)  # resulting portfolio return
port_vol(optimals)  # resulting portfolio volatility
port_ret(optimals) / port_vol(optimals)  # maximum Sharpe ratio

# minimize variance
optv = sco.minimize(port_vol,
                    eweights,
                    method='SLSQP',
                    bounds=bnds,
                    constraints=cons)

optimals = optv['x'].round(3)  # optimal portfolio weights
port_ret(optimals)  # resulting portfolio return
port_vol(optimals)  # resulting portfolio volatility
port_ret(optimals) / port_vol(optimals)  # maximum Sharpe ratio
#+end_src

** Efficient Frontier

The derivation of all optimal portfolios, i.e., all portfolios with minimum volatility for a given target return level (or all portfolios with maximum return for a given risk level) is similar to the previous optimizations. The only difference is that one has to iterate over multiple starting conditions.

#+begin_src python
trets = np.linspace(0.05, 0.2, 50)

# the two binding constraints for the efficient frontier
cons = ({'type': 'eq', 'fun': lambda x: port_ret(x) - tret},
        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})

bnds = tuple((0, 1) for x in weights)

tvols = []
for tret in trets:
    # minimization of portfolio volatility for different target returns
    res = sco.minimize(port_vol,
                       eweights,
                       method='SLSQP',
                       bounds=bnds,
                       constraints=cons)
    tvols.append(res['fun'])
tvols = np.array(tvols)

# minimum risk portfolios for given return levels (efficient frontier)
plt.figure()
plt.scatter(pvols, prets, c=prets/pvols,
            marker='.', alpha=0.8, cmap='coolwarm')
plt.plot(tvols, trets, 'b', lw=2.0)
plt.plot(port_vol(opts['x']),
         port_ret(opts['x']),
         'y*',
         markersize=15.0)
plt.plot(port_vol(optv['x']),
         port_ret(optv['x']),
         'r*',
         markersize=15.0)
plt.xlabel('expected volatility')
plt.ylabel('expected return')
plt.colorbar(label='Sharpe Ratio')
plt.show()
#+end_src

** Capital Market Line

In additional to financial instruments like stocks or commodities, there is a general, universal investment opportunity: cash or cash accounts.

Taking into account such a riskless asset enhances the efficient investment opportunity set for investors considerably. The basic idea is that investors first determine an efficient portfolio of risky assets and then add the riskless asset to the mix.

By adjusting the proportion of the investor's wealth to be invested in the riskless asset, it is possible to achieve any risk-return profile that lies on the straight line (in the risk-return space) between the riskless asset and the efficient portfolio.

The optimal portfolio is the one where the tangent line of the efficient frontier goes exactly through the risk-return point of the riskless portfolio. For example, consider a riskless interest rate of $r_f = 0.01$. The portfolio is to be found on the efficient frontier for which the tangent goes through the point $(\sigma_f, r_f) = (0, 0.01)$ in risk-return space.

#+begin_src python
import scipy.interpolate as sci

ind = np.argmin(tvols)  # index position of minimum volatility portfolio
evols = tvols[ind:]     # relevant portfolio volatility
erets = trets[ind:]     # relevant portfolio returns

tck = sci.splrep(evols, erets)

def f(x):
    """Efficient frontier function (spline approximation)"""
    return sci.splev(x, tck, der=0)

def df(x):
    """First derivative of efficient frontier function"""
    return sci.splev(x, tck, der=1)

def equations(p, rf=0.01):
    # equations describing the capital market line
    eq1 = rf - p[0]
    eq2 = rf + p[1] * p[2] - f(p[2])
    eq3 = p[1] - df(p[2])
    return eq1, eq2, eq3

opt = sco.fsolve(equations, [0.01, 0.5, 0.15])

np.round(equations(opt), 6)

# Capital market line and tangent portfolio (star) for risk-free rate of 1%
plt.figure()
plt.scatter(pvols, prets, c=(prets - 0.01)/pvols,
            marker='.', cmap='coolwarm')
plt.plot(evols, erets, 'b', lw=2.0)
cx = np.linspace(0.0, 0.3)
plt.plot(cx, opt[0] + opt[1] * cx, 'r', lw=1.5)
plt.plot(opt[2], f(opt[2]), 'y*', markersize=15.0)
plt.grid(True)
plt.axhline(0, color='k', ls='--', lw=2.0)
plt.axvline(0, color='k', ls='--', lw=2.0)
plt.xlabel('expected volatility')
plt.ylabel('expected return')
plt.colorbar(label='Sharpe Ratio')
plt.show()

# get portfolio weights of the optimal (tangent) portfolio
cons = ({'type': 'eq', 'fun': lambda x: port_ret(x) - f(opt[2])},
        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
res = sco.minimize(port_vol, eweights, method='SLSQP', bounds=bnds, constraints=cons)

res['x'].round(3)

port_ret(res['x'])
port_vol(res['x'])
port_ret(res['x']) / port_vol(res['x'])
#+end_src

* Bayesian Statistics

** Bayes' Formula

The most common interpretation of Bayes' formula in finance is the /diachronic interpretation/. This mainly states that over time one learns new information about certain variables or parameters of interest, like the mean return of a time series.

Bayes' Formula
$P(H | D) = \frac{p(H) p(D | H)}{p(D)}$

Here, $H$ stands for an event, the hypothesis, and $D$ represents the data an experiment or the real world might present.

On the basis of these notions, we have:

- $p(H)$ : the prior probability
- $p(D)$ : the probability for the data under any hypothesis, called the /normalizing constant/
- $p(D | H)$ : the /likelihood/ (i.e., the probability) of the data under hypothesis $H$
- $p(H | D)$ : the posterior probability; i.e., after one has seen the data


** Bayesian Regression

With PyMC3 the python ecosystem provides a comprehensive package to technically implement Bayesian statistics and probabilistic programming.

Consider the following example based on noisy data around a straight line.

First, an ordinary least squares regression:

#+begin_src python
x = np.linspace(0, 10, 500)
y = 4 + 2 * x + np.random.standard_normal(len(x)) * 2
reg = np.polyfit(x, y, 1)

# sample data points and a regression line
plt.figure()
plt.scatter(x, y, c=y, marker='v', cmap='coolwarm')
plt.plot(x, reg[1] + reg[0] * x, lw=2.0)
plt.colorbar()
plt.xlabel('x')
plt.ylabel('y')
plt.show()
#+end_src

And another using PyMC3 for Bayesian Regression

$\hat{y}(x) = \alpha + \beta \cdot x$
$\alpha \sim \mathcal{N}(0, 20)$
$\beta \sim \mathcal{N}(0, 10)$

- find_MAP() finds the starting point for the sampling algorithm by deriving the /local maximum a posteriori point/.
- NUTS() implements the so-called "efficient No-U-Turn Sampler with dual averaging" (NUTS) algorithm for MCMC sampling given assumed priors
- sample() draws a number of samples given the starting value from find_MAP() and the optimal step size from the NUTS algorithm

#+begin_src python
import pymc3 as pm

with pm.Model() as model:
    # model
    # define priors
    alpha = pm.Normal('alpha', mu=0, sd=20)
    beta = pm.Normal('beta', mu=0, sd=20)
    sigma = pm.Uniform('sigma', lower=0, upper=10)
    # define model specification (linear regression)
    y_est = alpha + beta * x
    # define likelihood
    likelihood = pm.Normal('y', mu=y_est, sd=sigma, observed=y)
    # inference
    # find starting value by optimization
    start = pm.find_MAP()
    # instantiate the MCMC algorithm
    step = pm.NUTS()
    # draw posterior samples using NUTS
    trace = pm.sample(100, tune=1000, start=start, progressbar=True)

# show summary statistics from samplings
pm.summary(trace)
# estimates from the first sample
trace[0]

# trace plot
pm.traceplot(trace, lines={'alpha': 4, 'beta': 2, 'sigma': 2})
plt.show()

# regression lines based on different estimates
plt.figure()
plt.scatter(x, y, c=y, marker='v', cmap='coolwarm')
plt.colorbar()
plt.xlabel('x')
plt.ylabel('y')
for i in range(len(trace)):
    plt.plot(x, trace['alpha'][i] + trace['beta'][i] * x)
#+end_src

** Two Financial Instruments

#+begin_src python
data = raw[['GDX', 'GLD']].dropna()
data = data / data.iloc[0]
# normalized prices for GLD and GDX over time
data.plot()
plt.show()

# scatter plot of GLD prices against GDX prices
mpl_dates = matplotlib.dates.date2num(data.index.to_pydatetime())
plt.figure()
plt.scatter(data['GDX'], data['GLD'], c=mpl_dates, marker='o', cmap='coolwarm')
plt.xlabel('GDX')
plt.ylabel('GLD')
plt.colorbar(ticks=matplotlib.dates.DayLocator(interval=250),
             format=matplotlib.dates.DateFormatter('%d %b %y'))
plt.show()

# implement a Bayesian regression on the basis of these two time series
with pm.Model() as model:
    alpha = pm.Normal('alpha', mu=0, sd=20)
    beta = pm.Normal('beta', mu=0, sd=20)
    sigma = pm.Uniform('sigma', lower=0, upper=50)
    y_est = alpha + beta * data['GDX'].values
    likelihood = pm.Normal('GLD', mu=y_est, sd=sigma, observed=data['GLD'].values)
    start = pm.find_MAP()
    step = pm.NUTS()
    trace = pm.sample(250, tune=2000, start=start, progressbar=True)


pm.summary(trace)
fig = pm.traceplot(trace)
plt.show()

# multiple Bayesian regression lines through GDX and GLD data
plt.figure()
plt.scatter(data['GDX'], data['GLD'], c=mpl_dates, marker='o', cmap='coolwarm')
plt.xlabel('GDX')
plt.ylabel('GLD')
for i in range(len(trace)):
    plt.plot(data['GDX'], trace['alpha'][i] + trace['beta'][i] * data['GDX'])
plt.colorbar(ticks=matplotlib.dates.DayLocator(interval=250),
             format=matplotlib.dates.DateFormatter('%d %b %y'))
plt.show()
#+end_src

** Updating Estimates Over Time

The Bayesian approach in finance is generally most useful when seen as diachronic -- i.e., in the sense that new data revealed over time allows for better regressions and estimates through updating or learning.

To incorporate this concept in the current example, assume that the regression parameters are not only random and distributed in some fashion, but that they follow some kind of /random walk/ over time.

To this end, we define a new PyMC3 model, this time specifying parameter values as random walks.

#+begin_src python
from pymc3.distributions.timeseries import GaussianRandomWalk

subsample_alpha, subsample_beta = 50, 50

model_randomwalk = pm.Model()
with model_randomwalk:
    # define priors for the random walk parameters
    sigma_alpha = pm.Exponential('sig_alpha', 1. / .02, testval=.1)
    sigma_beta = pm.Exponential('sig_beta', 1. / .02, testval=.1)
    # models for the random walks
    alpha = GaussianRandomWalk('alpha', sigma_alpha ** -2,
                               shape=int(len(data) / subsample_alpha))
    beta = GaussianRandomWalk('beta', sigma_beta ** -2,
                               shape=int(len(data) / subsample_beta))
    # brings the parameter vectors to interval length
    alpha_r = np.repeat(alpha, subsample_alpha)
    beta_r = np.repeat(beta, subsample_beta)
    # defines the regression model
    regression = alpha_r + beta_r * data['GDX'][:1950].values[:2100]
    # the prior for the standard deviation
    sd = pm.Uniform('sd', 0, 20)
    # defines the likelihood with mu from regression results
    likelihood = pm.Normal('GLD', mu=regression, sd=sd, observed=data['GLD'][:1950].values[:2100])


with model_randomwalk:
    start = pm.find_MAP(vars=[alpha, beta])
    step = pm.NUTS(scaling=start)
    trace_rw = pm.sample(250, tune=1000, start=start, progressbar=True)

pm.summary(trace_rw).head()  # the summary statistics per interval

sh = np.shape(trace_rw['alpha'])
sh  # shape of the object with parameter estimates

# creates a list of dates to match the number of intervals
part_dates = np.linspace(min(mpl_dates), max(mpl_dates), sh[1])

from datetime import datetime

index = [datetime.fromordinal(int(date)) for date in part_dates]

# collects the relevant parameter time series in two DataFrame objects
alpha = {'alpha_%i' % i: v for i, v in enumerate(trace_rw['alpha']) if i < 20}

beta = {'beta_%i' % i: v for i, v in enumerate(trace_rw['beta']) if i < 20}

df_alpha = pd.DataFrame(alpha, index=index)
df_beta = pd.DataFrame(beta, index=index)

ax = df_alpha.plot(color='b', style='-.', legend=False, lw=0.7)
df_beta.plot(color='r', style='-.', legend=False, lw=0.5, ax=ax)
plt.ylabel('alpha/beta')
plt.show()

# scatter plot with time-dependent regression lines (updated estimates)
plt.figure()
plt.scatter(data['GDX'], data['GLD'], c=mpl_dates, marker='o', cmap='coolwarm')
plt.colorbar(ticks=matplotlib.dates.DayLocator(interval=250),
             format=matplotlib.dates.DateFormatter('%d %b %y'))
plt.xlabel('GDX')
plt.ylabel('GLD')
x = np.linspace(min(data['GDX']), max(data['GDX']))
for i in range(sh[1]):
    alpha_rw = np.mean(trace_rw['alpha'].T[i])
    beta_rw = np.mean(trace_rw['beta'].T[i])
    plt.plot(x, alpha_rw + beta_rw * x, '--', lw=0.7, color=plt.cm.coolwarm(i / sh[1]))
plt.show()
#+end_src

* Machine Learning

** Unsupervised Learning

#+begin_src python
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

X, y = make_blobs(n_samples=250, centers=4, random_state=500, cluster_std=1.25)

# sample data for the application of clustering algorithms
plt.figure()
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.show()

# k-means clustering
from sklearn.cluster import KMeans

model = KMeans(n_clusters=4, random_state=0)
model.fit(X)

y_kmeans = model.predict(X)

plt.figure()
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='coolwarm')
plt.show()

# Gaussian mixture
from sklearn.mixture import GaussianMixture

model = GaussianMixture(n_components=4, random_state=0)
model.fit(X)

y_gm = model.predict(X)
(y_gm == y_kmeans).all()

plt.figure()
plt.scatter(X[:, 0], X[:, 1], c=y_gm, cmap='coolwarm')
plt.show()
#+end_src

** Supervised Learning

#+begin_src python
from sklearn.datasets import make_classification

n_samples = 100

X, y = make_classification(n_samples=n_samples,
                           n_features=2,
                           n_informative=2,
                           n_redundant=0,
                           n_repeated=0,
                           random_state=250)

plt.figure()
plt.hist(X)
plt.show()
plt.figure()
plt.scatter(x=X[:, 0], y=X[:, 1], c=y, cmap='coolwarm')
plt.show()
#+end_src

*** Gaussian Naive Bayes

#+begin_src python
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

model = GaussianNB()
model.fit(X, y)

model.predict_proba(X).round(4)[:5]

pred = model.predict(X)

accuracy_score(y, pred)

Xc = X[y == pred]
Xf = X[y != pred]
plt.figure()
# correct predictions
plt.scatter(x=Xc[:, 0], y=Xc[:, 1], c=y[y == pred], marker='o', cmap='coolwarm')
# false predictions
plt.scatter(x=Xf[:, 0], y=Xf[:, 1], c=y[y != pred], marker='x', cmap='coolwarm')
plt.show()
#+end_src

*** Logistic Regression

#+begin_src python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(C=1, solver='lbfgs')
model.fit(X, y)

model.predict_proba(X).round(4)[:5]

pred = model.predict(X)

accuracy_score(y, pred)

Xc = X[y == pred]
Xf = X[y != pred]
plt.figure()
# correct predictions
plt.scatter(x=Xc[:, 0], y=Xc[:, 1], c=y[y == pred], marker='o', cmap='coolwarm')
# false predictions
plt.scatter(x=Xf[:, 0], y=Xf[:, 1], c=y[y != pred], marker='x', cmap='coolwarm')
plt.show()
#+end_src

*** Decision Trees

#+begin_src python
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(max_depth=1)
model.fit(X, y)

pred = model.predict(X)
accuracy_score(y, pred)


print('{:>8s} | {:8s}'.format('depth', 'accuracy'))
print(20 * '-')
for depth in range(1, 7):
    model = DecisionTreeClassifier(max_depth=depth)
    model.fit(X, y)
    acc = accuracy_score(y, model.predict(X))
    print('{:8d} | {:8.2f}'.format(depth, acc))
#+end_src

*** Neural Networks

#+begin_src python
# DNN with scikit-learn
from sklearn.neural_network import MLPClassifier

model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=2 * [75], random_state=10)
model.fit(X, y)
pred = model.predict(X)
accuracy_score(y, pred)
#+end_src

#+begin_src python
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
from cytoolz import juxt

x_train, y_train = X[:80], y[:80]
x_test, y_test = X[80:], y[80:]

scaler = StandardScaler()
x_train_s = scaler.fit_transform(x_train)
x_test_s = scaler.fit_transform(x_test)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    loss=tf.keras.losses.binary_crossentropy,
    optimizer=tf.keras.optimizers.Adam(lr=0.03),
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy'),
        tf.keras.metrics.Precision(name='precision'),
        tf.keras.metrics.Recall(name='recall')
    ]
)

history = model.fit(x_train_s, y_train, epochs=100)

plt.plot(np.arange(1, 101), history.history['loss'], label='Loss')
plt.plot(np.arange(1, 101), history.history['accuracy'], label='Accuracy')
plt.plot(np.arange(1, 101), history.history['precision'], label='Precision')
plt.plot(np.arange(1, 101), history.history['recall'], label='Recall')
plt.title('Evaluation metrics', size=20)
plt.xlabel('Epoch', size=14)
plt.legend()
plt.show()

preds = model.predict(x_test_s)
pred_classes = [1 if prob > 0.5 else 0 for prob in np.ravel(preds)]

juxt(confusion_matrix, accuracy_score, precision_score, recall_score)(y_test, pred_classes)
#+end_src

** Feature Transforms

#+begin_src python
from sklearn import preprocessing

# standard normally distributed data with 0 mean and unit variance
xs = preprocessing.StandardScaler().fit_transform(X)
# xform to a given range for every feature as defined by the min and max values per feature
xm = preprocessing.MinMaxScaler().fit_transform(X)
# scale to the unit norm (L1 or L2)
xl1 = preprocessing.Normalizer(norm='l1').fit_transform(X)
xl2 = preprocessing.Normalizer(norm='l2').fit_transform(X)

plt.figure()
markers = ['o', '.', 'x', '^', 'v']
data_sets = [X, xs, xm, xl1, xl2]
labels = ['raw', 'standard', 'minmax', 'norm(1)', 'norm(2)']
for x, m, l in zip(data_sets, markers, labels):
    plt.scatter(x=x[:, 0], y=x[:, 1], c=y, marker=m, cmap='coolwarm', label=l)
plt.legend()
plt.show()

# for categorical features
xb = preprocessing.Binarizer().fit_transform(X)
xd = np.digitize(X, bins=[-1, 0, 1])
#+end_src

** Cross Validation

#+begin_src python
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)
model = SVC(C=1, kernel='linear')
model.fit(train_x, train_y)

pred_train = model.predict(train_x)
accuracy_score(train_y, pred_train)

pred_test = model.predict(test_x)
accuracy_score(test_y, pred_test)

print('{:>8s} | {:8s}'.format('kernel', 'accuracy'))
print(20 * '-')
for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:
    model = SVC(C=1, kernel=kernel, gamma='auto')
    model.fit(train_x, train_y)
    acc = accuracy_score(test_y, model.predict(test_x))
    print('{:>8s} | {:8.3f}'.format(kernel, acc))
#+end_src
