#+TITLE: Chapter 13: Statistics

This chapter has 4 focal points:

- Normality Tests

A large number of important financial models, like modern or mean-variance portfolio theory (MPT) and the capital asset pricing model (CAPM) rest on the assumption that returns of securities are normally distributed. This chapter presents approaches to testing a given time series for normality of returns

- Portfolio Optimization

Starting in the early 1950s with the work of pioneer Harry Markowitz, *Mean-Variance Portfolio Theory* began to replace people's reliance on judgement and experience with rigorous mathematical and statistical methods when it comes to the investment of money in financial markets.

- Bayesian Statistics

Bayesian statistics introduces the notion of beliefs of agents and the updating of beliefs to statistics.

- Machine Learning

This section looks at supervised learning for classification

* Normality Tests

Among others, the following cornerstones of financial theory rest to a large extend on the assumption that returns of a financial instrument are normally distributed:

- Portfolio Theory
  When stock returns are normally distributed, optimal portfolio choice can be cast into a setting where only the (expected) /mean return/ and the /variance of the returns/ (or the volatility) as well as the /covariances/ between different stocks are relevant for an investment decision (i.e., an optimal portfolio composition).

- Capital Asset Pricing Model
  When stock returns are normally distributed, prices of single stocks can be elegantly expressed in a linear relationship to a broad market index; the relationship is generally expressed by a measure for the co-movement of a single stock with the market index called beta or $\beta$

- Efficient Markets Hypothesis
  An efficient market is a market where prices reflect all available information, where "all" can be defined more narrowly or more widely (e.g., as in "all publicly available" information vs. including also "only privately available" information). If this hypothesis holds true, then stock prices fluctuate randomly and returns are normally distributed

- Option pricing theory
  Brownian motion is /the/ benchmark model for the modeling of random price movements of financial instruments; the famous Black-Scholes-Merton option pricing formula uses a geometric Brownian motion as the model for a stock's random price fluctuations over time, leading to log-normally distributed prices and normally distributed returns.

** Benchmark Case

The analysis starts with GBM as one of the canonical stochastic processes used in financial modeling.

The following can be said about the characteristics of paths from a geometric Brownian motion $S$:

- Normal log returns

Log returns $\log{\frac{S_t}{S_s}} = \log{S_t} - \log{S_s}$ between two times $0 < s < t$ are normally distributed

- Log-normal values

At any time $t > 0$, the values $S_t$ are log-normally distributed.

#+begin_src python
import math
import numpy as np
import scipy.stats as scs
import statsmodels.api as sm
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('tkAgg')
plt.style.use('seaborn')

def gen_paths(S0, r, sigma, T, M, I):
    """
    Generate Monte Carlo paths for geometric Brownian motion

    Args:
        S0: initial stock/index value
        r : constant short rate
        sigma: constant volatility
        T : final time horizon
        M : number of time steps / intervals
        I : number of paths to be simulated

    Returns:
        paths: simulated paths given the parameters
    """
    dt = T / M
    paths = np.zeros((M + 1, I))
    paths[0] = S0
    for t in range(1, M + 1):
        rand = np.random.standard_normal(I)
        # standardize / match 1st and 2nd moment
        rand = (rand - rand.mean()) / rand.std()
        # vectorized Euler discretization of geometric Brownian motion
        paths[t] = paths[t - 1] * np.exp((r - 0.5 * sigma ** 2) * dt +
                                         sigma * math.sqrt(dt) * rand)
    return paths

S0 = 100.
r = 0.05
sigma = 0.2
T = 1.0
M = 50
I = 250000
np.random.seed(1000)

paths = gen_paths(S0, r, sigma, T, M, I)

S0 * math.exp(r * T)  # expected value and average simulated value

paths[-1].mean()

# ten simulated paths of geometric Brownian motion
plt.figure()
plt.plot(paths[:, :10])
plt.xlabel('time steps')
plt.ylabel('index level')
plt.show()

log_returns = np.log(paths[1:] / paths[:-1]).round(4)

def print_statistics(array):
    """
    prints selected statistics
    """
    sta = scs.describe(array)
    print('%14s %15s' % ('statistic', 'value'))
    print(30 * '-')
    print('%14s %15.5f' % ('size', sta[0]))
    print('%14s %15.5f' % ('min', sta[1][0]))
    print('%14s %15.5f' % ('max', sta[1][1]))
    print('%14s %15.5f' % ('mean', sta[2]))
    print('%14s %15.5f' % ('std', np.sqrt(sta[3])))
    print('%14s %15.5f' % ('skew', sta[4]))
    print('%14s %15.5f' % ('kurtosis', sta[5]))

print_statistics(log_returns.flatten())

log_returns.mean() * M + 0.5 * sigma ** 2  # annualized mean log return after correction for the Ito term

log_returns.std() * math.sqrt(M)  # annualized volatility, i.e., annualized std dev of log returns

# histogram of log returns of geometric Brownian motion and normal density function
plt.figure()
plt.hist(log_returns.flatten(), bins=70, density=True, label='frequency', color='b')
plt.xlabel('log return')
plt.ylabel('frequency')
x = np.linspace(plt.axis()[0], plt.axis()[1])
plt.plot(x, scs.norm.pdf(x, loc=r/M, scale=sigma/np.sqrt(M)))
plt.show()
#+end_src

Comparing a frequency distribution (histogram) with a theoretical PDF is not the only way to graphically "test" for normality. Quantile-quantile (QQ) plots are also well-suited for this task.

#+begin_src python
sm.qqplot(log_returns.flatten()[::500], line='s')
plt.xlabel('theoretical quantiles')
plt.ylabel('sample quantiles')
plt.show()
#+end_src

However appealing graphical tests are, they generally cannot replace more rigorous testing procedures.

- skewness test (skewtest())
  tests whether the skew (3rd moment) of the sample data is normal
- kurtosis test (kurtosistest())
- normality test (normaltest())

#+begin_src python
def normality_tests(arr):
    """
    Tests for normality distribution of a given data set
    Args:
        arr: object to generate statistics on
    Side Effect:
        prints a bunch of stats to the console
    """
    print(f'Skew:\t {scs.skew(arr):30.3f}')
    print(f'Skew test p_value:\t {scs.skewtest(arr)[1]:14.3f}')
    print(f'Kurt of data set:\t {scs.kurtosis(arr):14.3f}')
    print(f'Kurt test p-value:\t {scs.kurtosistest(arr)[1]:14.3f}')
    print(f'Norm test p-value:\t {scs.normaltest(arr)[1]:14.3f}')

normality_tests(log_returns.flatten())

# histogram of simualted end-of-period index levels for geometric Brownian motion
f, (ax1, ax2) = plt.subplots(1, 2)
ax1.hist(paths[-1], bins=30)
ax1.set_xlabel('index level')
ax1.set_ylabel('frequency')
ax1.set_title('regular data')
ax2.hist(np.log(paths[-1]), bins=30)
ax2.set_xlabel('log index level')
ax2.set_title('log data')
plt.show()
#+end_src


** Real-World Data

This section analyzes 4 historical financial time series: apple, microsoft, S&P 500, SPDR Gold

#+begin_src python
import pandas as pd
raw = pd.read_csv('data/tr_eikon_eod_data.csv',
                  index_col=0, parse_dates=True).dropna()
symbols = ['SPY', 'GLD', 'AAPL.O', 'MSFT.O']

data = raw[symbols]

data.info()
data.head()

# normalized prices of financial instruments over time
(data / data.iloc[0] * 100).plot()
plt.show()

# histograms of log returns for financial instruments
log_returns = np.log(data / data.shift(1))
log_returns.head()

log_returns.hist(bins=50)
plt.show()

for sym in symbols:
    print(f'\nResults for symbol {sym}')
    print(30 * '-')
    log_data = np.array(log_returns[sym].dropna())
    print_statistics(log_data)
    print('\nNormality Tests\n')
    normality_tests(log_data)
#+end_src

* Portfolio Optimization

** The Data

The basic idea of MPT is to make use of /diversification/ to achieve a minimal portfolio risk given a target return level or a maximum portfolio return given a certain level of risk. One would expect such diversification effects for the right combination of a larger number of assets and a certain diversity in the assets.

#+begin_src python
noa = len(symbols)  # number of assets

rets = np.log(data / data.shift(1))
rets.hist(bins=40)
plt.show()
#+end_src

The /covariance matrix/ for the financial instruments to be invested in is the central piece of the portfolio selection process.

#+begin_src python
rets.mean() * 252  # annualized mean returns
rets.cov() * 252   # annualized covariance matrix
#+end_src

** The Basic Theory

In what follows, it is assumed that an investor is not allowed to set up short positions in a financial instrument. Only long positions are allowed, which implies that 100% of the investor's wealth has to be divided among the available instruments in such a way that all positions are long (positive) and that the positions add up to 100%.

#+begin_src python
# gen 4 uniformly distributed random nums and normalize s.t. sum(rvs) = 1
weights = np.random.random(noa)
weights /= np.sum(weights)
#+end_src

General formula for expected portfolio return

$\mu_p = E(\Sigma_I w_i r_i) = w^T \mu$

#+begin_src python
np.sum(rets.mean() * weights) * 252  # annualized portfolio return given the portfolio weights
#+end_src

The second object of importance in MPT is the /expected portfolio variance/.

General formula for expected portfolio variance
$\sigma_p^2 = E((r - \mu)^2) = w^T \Sigma w$

#+begin_src python
portfolio_cov = np.dot(weights.T, np.dot(rets.cov() * 252, weights))
volatility = math.sqrt(portfolio_cov)
#+end_src

Of paramount interest to investors is what risk-return profiles are possible for a given set of financial instruments, and their statistical characteristics.

The following implements a Monte Carlo simulation to generate random portfolio weight vectors on a larger scale. For every simulated allocation, the code records the resulting expected portfolio return and variance.

#+begin_src python
def port_ret(weights):
    return np.sum(rets.mean() * weights) * 252

def port_vol(weights):
    return np.sqrt(np.dot(weights.T, np.dot(rets.cov() * 252, weights)))

# monte carlo simulation of portfolio weights
prets, pvols = [], []
for p in range(2500):
    weights = np.random.random(noa)
    weights /= np.sum(weights)
    prets.append(port_ret(weights))
    pvols.append(port_vol(weights))
prets = np.array(prets)
pvols = np.array(pvols)

# expected return and volatility for random portfolio weights
plt.figure()
plt.scatter(pvols, prets, c=prets / pvols,
            marker='o', cmap='coolwarm')
plt.xlabel('expected volatility')
plt.ylabel('expected return')
plt.colorbar(label='Sharpe ratio')
plt.show()
#+end_src

The plot above illustrates the results of the Monte Carlo simulation. It also provides results for the *Sharpe ratio*, defined as:

$SR \equiv \frac{\mu_p - r_f}{\sigma_p}$

i.e., the expected excess return of the portfolio over the risk-free short rate divided by the expected standard deviation of the portfolio.
For simplicity, in this casae $r_f \equiv 0$ is assumed.

As an investor, one is generally interested in the maximum return given a fixed risk level or the minimum risk given a fixed return expectation.
This set of portfolios then makes up the /efficient frontier/.

** Optimal Portfolios

This /minimization/ function is quite general and allows for equality constraints, inequality constrants, and numerical bounds for the parameters.

First, the /maximization of the Sharpe ratio/. Formally, the negative value of the Sharpe ratio is minimized to derive at the maximum value and the optimal portfolio composition. The constraint is that all the weights add up to 1.

#+begin_src python
import scipy.optimize as sco

def min_func_sharpe(weights):
    # function to be minimized
    return -port_ret(weights) / port_vol(weights)

cons = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})  # equality constraint
bnds = tuple((0, 1) for x in range(noa))  # bounds for the parameters
eweights = np.array(noa * [1. / noa,])  # equal weights vector

opts = sco.minimize(min_func_sharpe,
                    eweights,
                    method='SLSQP',
                    bounds=bnds,
                    constraints=cons)

# maximize Sharpe ratio
optimals = opts['x'].round(3)  # optimal portfolio weights
port_ret(optimals)  # resulting portfolio return
port_vol(optimals)  # resulting portfolio volatility
port_ret(optimals) / port_vol(optimals)  # maximum Sharpe ratio

# minimize variance
optv = sco.minimize(port_vol,
                    eweights,
                    method='SLSQP',
                    bounds=bnds,
                    constraints=cons)

optimals = optv['x'].round(3)  # optimal portfolio weights
port_ret(optimals)  # resulting portfolio return
port_vol(optimals)  # resulting portfolio volatility
port_ret(optimals) / port_vol(optimals)  # maximum Sharpe ratio
#+end_src

** Efficient Frontier

The derivation of all optimal portfolios, i.e., all portfolios with minimum volatility for a given target return level (or all portfolios with maximum return for a given risk level) is similar to the previous optimizations. The only difference is that one has to iterate over multiple starting conditions.

#+begin_src python
trets = np.linspace(0.05, 0.2, 50)

# the two binding constraints for the efficient frontier
cons = ({'type': 'eq', 'fun': lambda x: port_ret(x) - tret},
        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})

bnds = tuple((0, 1) for x in weights)

tvols = []
for tret in trets:
    # minimization of portfolio volatility for different target returns
    res = sco.minimize(port_vol,
                       eweights,
                       method='SLSQP',
                       bounds=bnds,
                       constraints=cons)
    tvols.append(res['fun'])
tvols = np.array(tvols)

# minimum risk portfolios for given return levels (efficient frontier)
plt.figure()
plt.scatter(pvols, prets, c=prets/pvols,
            marker='.', alpha=0.8, cmap='coolwarm')
plt.plot(tvols, trets, 'b', lw=2.0)
plt.plot(port_vol(opts['x']),
         port_ret(opts['x']),
         'y*',
         markersize=15.0)
plt.plot(port_vol(optv['x']),
         port_ret(optv['x']),
         'r*',
         markersize=15.0)
plt.xlabel('expected volatility')
plt.ylabel('expected return')
plt.colorbar(label='Sharpe Ratio')
plt.show()
#+end_src

** Capital Market Line

In additional to financial instruments like stocks or commodities, there is a general, universal investment opportunity: cash or cash accounts.

Taking into account such a riskless asset enhances the efficient investment opportunity set for investors considerably. The basic idea is that investors first determine an efficient portfolio of risky assets and then add the riskless asset to the mix.

By adjusting the proportion of the investor's wealth to be invested in the riskless asset, it is possible to achieve any risk-return profile that lies on the straight line (in the risk-return space) between the riskless asset and the efficient portfolio.

The optimal portfolio is the one where the tangent line of the efficient frontier goes exactly through the risk-return point of the riskless portfolio.
