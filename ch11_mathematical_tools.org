#+TITLE: Chapter 11: Mathematical Tools

This chapter covers:
- Approximation
- Convex Optimization
- Integration
- Symbolic Computation

* Approximation

#+begin_src python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('seaborn')
import matplotlib
matplotlib.use('tkAgg')

def f(x):
    return np.sin(x) + 0.5 * x

def create_plot(x, y, styles, labels, axlabels):
    plt.figure()
    for i in range(len(x)):
        plt.plot(x[i], y[i], styles[i], label=labels[i])
        plt.xlabel(axlabels[0])
        plt.ylabel(axlabels[1])
    plt.legend(loc=0)

x = np.linspace(-2 * np.pi, 2 * np.pi, 50)

create_plot([x], [f(x)], ['b'], ['f(x)'], ['x', 'f(x)'])
plt.show()
#+end_src


* Regression

Minimization problem of regression

$\min_{\alpha_1, ..., \alpha_n} \frac{1}{I} \Sigma_{i=1}^I (y_i - \Sigma_{d=1}^D \alpha_d \cdot b_d(x_i))^2$

The author mentions the use of monomial functions as basis functions

#+begin_src python
# monomials as basis functions
res = np.polyfit(x, f(x), deg=1, full=True)   # linear regression step
ry = np.polyval(res[0], x)                    # evaluation using the regression params

create_plot([x, x], [f(x), ry], ['b', 'r.'], ['f(x)', 'regression'], ['x', 'f(x)'])
plt.show()

# again, with a polynomial of degree 5
res = np.polyfit(x, f(x), deg=5, full=True)   # linear regression step
ry = np.polyval(res[0], x)                    # evaluation using the regression params

create_plot([x, x], [f(x), ry], ['b', 'r.'], ['f(x)', 'regression'], ['x', 'f(x)'])
plt.show()

# again, with a polynomial of degree 7
reg = np.polyfit(x, f(x), 7)
ry = np.polyval(reg, x)

np.allclose(f(x), ry)

np.mean((f(x) - ry) ** 2)

create_plot([x, x], [f(x), ry], ['b', 'r.'], ['f(x)', 'regression'], ['x', 'f(x)'])
plt.show()

# individual basis functions
matrix = np.zeros((3 + 1, len(x)))
matrix[3, :] = x ** 3
matrix[2, :] = x ** 2
matrix[1, :] = x
matrix[0, :] = 1

reg = np.linalg.lstsq(matrix.T, f(x), rcond=None)[0]

ry = np.dot(reg, matrix)

create_plot([x, x], [f(x), ry], ['b', 'r.'],
            ['f(x)', 'regression'], ['x', 'f(x)'])
plt.show()

# we can exploit previous knowledge. We know that there is a sin in the function
# therefore, we can include that information in the basis functions
matrix[3, :] = np.sin(x)

reg = np.linalg.lstsq(matrix.T, f(x), rcond=None)[0]

ry = np.dot(reg, matrix)

np.allclose(f(x), ry)

np.mean((f(x) - ry) ** 2)

create_plot([x, x], [f(x), ry], ['b', 'r.'],
            ['f(x)', 'regression'], ['x', 'f(x)'])
plt.show()

# noisy data
xn = np.linspace(-2 * np.pi, 2 * np.pi, 50)
xn = xn + 0.15 * np.random.standard_normal(len(xn))
yn = f(xn) + 0.25 * np.random.standard_normal(len(xn))

reg = np.polyfit(xn, yn, 7)
ry = np.polyval(reg, xn)

create_plot([x, x], [f(x), ry], ['b', 'r.'],
            ['f(x)', 'regression'], ['x', 'f(x)'])
plt.show()

# unsorted data
xu = np.random.rand(50) * 4 * np.pi - 2 * np.pi
yu = f(xu)

reg = np.polyfit(xu, yu, 5)
ry = np.polyval(reg, xu)

create_plot([xu, xu], [yu, ry], ['b.', 'ro'],
            ['f(x)', 'regression'], ['x', 'f(x)'])
plt.show()

# multiple dimensions
def fm(p):
    x, y = p
    return np.sin(x) + 0.25 * x + np.sqrt(y) + 0.05 * y ** 2

x = np.linspace(0, 10, 20)
y = np.linspace(0, 10, 20)
X, Y = np.meshgrid(x, y)

Z = fm((X, Y))
x = X.flatten()
y = Y.flatten()

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_surface(X, Y, Z, rstride=2, cstride=2,
                       cmap='coolwarm', linewidth=0.5,
                       antialiased=True)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
fig.colorbar(surf, shrink=0.5, aspect=5)
plt.show()

# we can also change the set of basis functions as well
matrix = np.zeros((len(x), 6 + 1))
matrix[:, 6] = np.sqrt(y)
matrix[:, 5] = np.sin(x)
matrix[:, 4] = y ** 2
matrix[:, 3] = x ** 2
matrix[:, 2] = y
matrix[:, 1] = x
matrix[:, 0] = 1

reg = np.linalg.lstsq(matrix, fm((x, y)), rcond=None)[0]

RZ = np.dot(matrix, reg).reshape((20, 20))

fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_surface(X, Y, Z, rstride=2, cstride=2,
                       cmap='coolwarm', linewidth=0.5,
                       antialiased=True)
surf2 = ax.plot_wireframe(X, Y, RZ, rstride=2, cstride=2, label='regression')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
fig.colorbar(surf, shrink=0.5, aspect=5)
plt.show()
#+end_src

* Interpolation

Remember that sorted (and non-noisy) data is required and that the approach is limited to low-dimensional problems. It is also comparatively computationally demanding compared to regression.

#+begin_src python
# here is a linear splines interpolation
import scipy.interpolate as spi

x = np.linspace(-2 * np.pi, 2 * np.pi, 25)

def f(x):
    return np.sin(x) + 0.5 * x

ipo = spi.splrep(x, f(x), k=1)

iy = spi.splev(x, ipo)

np.allclose(f(x), iy)

create_plot([x, x], [f(x), iy], ['b', 'ro'],
            ['f(x)', 'interpolation'], ['x', 'f(x)'])
plt.show()


#
xd = np.linspace(1.0, 3.0, 50)
iyd = spi.splev(xd, ipo)

create_plot([xd, xd], [f(xd), iyd], ['b', 'ro'],
            ['f(x)', 'interpolation'], ['x', 'f(x)'])
plt.show()

# repetition of the exercise, using cubic splines
ipo = spi.splrep(x, f(x), k=3)
iyd = spi.splev(xd, ipo)

np.allclose(f(xd), iyd)

np.mean((f(xd) - iyd) ** 2)

create_plot([xd, xd], [f(xd), iyd], ['b', 'ro'],
            ['f(x)', 'interpolation'], ['x', 'f(x)'])
plt.show()
#+end_src

* Convex Optimization

#+begin_src python
def fm(p):
    def fn(v):
        return np.sin(v) + 0.05 * v ** 2
    x, y = p
    return fn(x) + fn(y)

x, y = np.linspace(-10, 10, 50), np.linspace(-10, 10, 50)
X, Y = np.meshgrid(x, y)
Z = fm((X, Y))

fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_surface(X, Y, Z, rstride=2, cstride=2,
                       cmap='coolwarm', linewidth=0.5,
                       antialiased=True)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('f(x, y)')
fig.colorbar(surf, shrink=0.5, aspect=5)
plt.show()
#+end_src

*** Global Optimization

#+begin_src python
import scipy.optimize as sco

def fo(p, output: bool = True):
    x, y = p
    z = np.sin(x) + 0.05 * x ** 2 + np.sin(y) + 0.05 * y ** 2
    if output:
        print(f'{x:8.4f} | {y:8.4f} | {z:8.4f}')
    return z


space = (-10, 10.1, 5)  # from, to, step-size
sco.brute(fo, (space, space), finish=None)

from cytoolz import curry
no_output_fo = curry(fo, output=False)

space = (-10, 10.1, 0.1)  # from, to, step-size
opt1 = sco.brute(no_output_fo, (space, space), finish=None)
print(f"The brute forced minima is: {opt1} which has the value {fm(opt1):.3f}")
#+end_src
*** Local Optimization

#+begin_src python
opt2 = sco.fmin(fo, opt1, xtol=0.001, ftol=0.001, maxiter=15, maxfun=20)
fm(opt2)

sco.fmin(fo, (2.0, 2.0), maxiter=250)
#+end_src

*** Constrained Optimization

Consider the utility maximization problem of an investor who can invest in 2 risky securities. Both securities cost q_a = q_b = 10 USD today. After one year, they have a payoff of 15 USD and 5 USD, respectively, in state d. Both states are equally likely. Denote the vector payoffs for the two securities by r_a and r_b respectively.

The investor has a budget of w_0 = 100 USD to invest and derives utility from future wealth according to the utility function u(w) = \sqrt{w} where w is the wealth available.

We can formulate our problem like so:

max_{a, b} E(u(w_1)) = p \sqrt{w_{1u}} + (1 - p) \sqrt{w_{1d}}
w_1 = a \cdot r_a + b \cdot r_b
w_0 \geq a \cdot q_a + b \cdot q_b
a, b \geq 0

#+begin_src python
import math

def Eu(p):
    """The function to be minimized in order to maximize utility"""
    def eq(v1, v2, c1, c2):
        return 0.5 * math.sqrt(v1 * c1 + v2 * c2)
    s, b = p
    return -(eq(s, b, 15, 5) + eq(s, b, 5, 12))

# the inequality constraint as a dict object
cons = ({'type': 'ineq',
         'fun': lambda p: 100 - p[0] * 10 - p[1] * 10})

# the boundary values for the parameters (chosen to be wide enough)
bnds = ((0, 1000), (0, 1000))

# the constrained optimization
result = sco.minimize(Eu, [5, 5], method='SLSQP', bounds=bnds, constraints=cons)

result['x']  # the optimal parameter values
-result['fun']  # the negative min function value as the optimal solution value
np.dot(result['x'], [10, 10])  # the budget constraint is binding; all wealth is invested
#+end_src

* Integration

In valuation and option pricing, integration is an important tool. Risk-neutral values of derivatives can be expressed in general as the discounted expectation of their payoff under the risk-neutral or martingale measure. The expectation in turn is a sum in the discrete case and an integral in the continuous case.

#+begin_src python
import scipy.integrate as sci

def f(x):
    return np.sin(x) + 0.5 * x


x = np.linspace(0, 10)
y = f(x)
a, b = 0.5, 9.5
Ix = np.linspace(a, b)
Iy = f(Ix)
#+end_src

** Numerical Integration

The scipy.integrate subpackage contains a selection of functions to numerically integrate a given mathematical function for upper and lower integration limits.

#+begin_src python
sci.fixed_quad(f, a, b)  # fixed Gaussian quadrature
sci.quad(f, a, b)        # adaptive quadrature
sci.romberg(f, a, b)     # Romberg integration

# takes in an input list or ndarray
xi = np.linspace(0.5, 9.5, 25)

sci.trapz(f(xi), xi)  # trapezoidal rule
sci.simps(f(xi), xi)  # Simpson's rule
#+end_src

** Integration by Simulation

#+begin_src python
for i in range(1, 20):
    np.random.seed(1000)
    x = np.random.random(i * 10) * (b - a) + a
    print(np.mean(f(x)) * (b - a))
#+end_src

* Symbolic Computation

** Basics

#+begin_src python
import sympy as sy

x = sy.Symbol('x')
y = sy.Symbol('y')

type(x)

sy.sqrt(x)

3 + sy.sqrt(x) - 4 ** 2

f = x ** 2 + 3 + 0.5 * x ** 2 + 3 / 2

sy.simplify(f)

sy.pretty(f)
#+end_src

** Equations

#+begin_src python
# solves equations of the form f = 0
sy.solve(x ** 2 - 1)

sy.solve(x ** 2 - 1 - 3)
#+end_src

** Integration and Differentiation

#+begin_src python
a, b = sy.symbols('a b')
I = sy.Integral(sy.sin(x) + 0.5 * x, (x, a, b))

print(sy.pretty(I))

int_func = sy.integrate(sy.sin(x) + 0.5 * x, x)

print(sy.pretty(int_func))

Fb = int_func.subs(x, 9.5).evalf()
Fa = int_func.subs(x, 0.5).evalf()

Fb - Fa

int_func_limits = sy.integrate(sy.sin(x) + 0.5 * x, (x, a, b))
print(sy.pretty(int_func_limits))

int_func_limits.subs({a: 0.5, b: 9.5}).evalf()

sy.integrate(sy.sin(x) + 0.5 * x, (x, 0.5, 9.5))
#+end_src

** Differentiation

#+begin_src python
int_func.diff()

f = (sy.sin(x) + 0.05 * x ** 2 + sy.sin(y) + 0.05 * y ** 2)  # symbolic version of the function

# the two partial derivatives derived
del_x = sy.diff(f, x)
del_y = sy.diff(f, y)

# educated guesses for the roots and resulting optimal values
xo = sy.nsolve(del_x, -1.5)
yo = sy.nsolve(del_y, -1.5)

# the global minimum function value
f.subs({x: xo, y: yo}).evalf()
#+end_src
